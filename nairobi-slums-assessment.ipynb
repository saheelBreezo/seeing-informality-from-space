{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing spatial characteristics of informal settlement in Nairobi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sentinel 2 L1C\n",
    "2. OSM Boundaries\n",
    "3. Slum Boundaries\n",
    "4. Google Building Footprints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the slum boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_slum = gpd.read_file(\"nairobi/slum-boundaries/slums_Nrb.shp\")\n",
    "gdf_slum.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the CRS and attributes in the shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CRS of the slum settlement shapefile\", gdf_slum.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attributes\")\n",
    "print(gdf_slum.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Area of Slum Settlements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_slum[\"area_acre\"] = gdf_slum[\"geometry\"].area / 4047\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the biggest 10 landfill settlement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_slum = gdf_slum.loc[gdf_slum[\"area_acre\"].nlargest(10).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the OSM buildings layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_osm_buildings = gpd.read_file(\"nairobi/osm/buildings.shp\")\n",
    "gdf_osm_buildings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting buildings within slum settlements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure building shapefile has the same crs as slums\n",
    "gdf_osm_buildings_reproject = gdf_osm_buildings.to_crs(gdf_slum.crs)\n",
    "\n",
    "# apply spatial inner join with within query to get list of buildings within each slum\n",
    "# resuls in a dataframe with a new column to buildings called object id of slums\n",
    "buildings_within_slums = gpd.sjoin(gdf_osm_buildings_reproject, gdf_slum[[\"OBJECTID\", \"geometry\"]],\n",
    "                how=\"inner\",\n",
    "                predicate=\"within\",\n",
    "                lsuffix=\"left\",\n",
    "                rsuffix=\"right\")\n",
    "\n",
    "# Group by slum ID and count the buildings\n",
    "building_counts = buildings_within_slums.groupby(\"OBJECTID\").size().reset_index(name=\"building_count\")\n",
    "\n",
    "# finally merge building counts with slums \n",
    "slums_with_counts = gdf_slum.merge(building_counts, on=\"OBJECTID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building footprint data can be too large to fit into memory, try loading data in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read buildings CSV in chunks and process each chunk\n",
    "def count_buildings_in_slums(slums, buildings_shp, crs, chunk_size=100000):\n",
    "    building_counts = gpd.GeoDataFrame()  # Initialize empty GeoDataFrame to store counts\n",
    "    \n",
    "    # Process buildings in chunks\n",
    "    for chunk in gpd.read_file(buildings_shp, chunksize=chunk_size):\n",
    "        # reproject the chunk to match slums crs\n",
    "        chunk = chunk.to_crs(crs)\n",
    "        \n",
    "        # Spatial join to find buildings within each slum\n",
    "        buildings_within_slums = gpd.sjoin(chunk, slums[[\"OBJECTID\", \"geometry\"]], how=\"inner\", predicate=\"within\")\n",
    "        \n",
    "        # Count buildings per slum in the current chunk\n",
    "        chunk_counts = buildings_within_slums.groupby(\"OBJECTID\").size().reset_index(name=\"building_count\")\n",
    "        \n",
    "        # Append to the main building counts DataFrame\n",
    "        building_counts = pd.concat([building_counts, chunk_counts], ignore_index=True)\n",
    "    \n",
    "    # Aggregate counts across chunks\n",
    "    building_counts = building_counts.groupby(\"OBJECTID\").sum().reset_index()\n",
    "    return building_counts\n",
    "\n",
    "# Calculate building counts and merge with slums GeoDataFrame\n",
    "buildings_shp = \"nairobi/osm/buildings.shp\"\n",
    "building_counts = count_buildings_in_slums(gdf_slum, buildings_shp)\n",
    "slums_with_counts = gdf_slum.merge(building_counts, on=\"OBJECTID\", how=\"left\")\n",
    "\n",
    "# Fill NaNs and set building count as integer\n",
    "slums_with_counts[\"building_count\"] = slums_with_counts[\"building_count\"].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the OSM Points Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gdf = gpd.read_file(\"nairobi/osm/points.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding toilets within 500m of slum boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 500\n",
    "\n",
    "toilets = points_gdf[points_gdf[\"type\"]==\"toilets\"]\n",
    "\n",
    "# Create a 500m buffer around each slum polygon\n",
    "gdf_slum['geometry'] = gdf_slum['geometry'].buffer(buffer_size)\n",
    "\n",
    "# Perform spatial join to find toilets within each buffered slum polygon\n",
    "joined_gdf = gpd.sjoin(toilets, gdf_slum[[\"OBJECTID\", \"geometry\"]], how='inner', predicate='within')\n",
    "\n",
    "# Count the toilets for each slum polygon\n",
    "counts = joined_gdf.groupby('OBJECTID').size()\n",
    "\n",
    "# Add the counts back to the slum_gdf\n",
    "gdf_slum[f\"toilet_within_{buffer_size}m\"] = gdf_slum.OBJECTID.map(counts).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same logic for bus stops, hospitals and schools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the OSM line layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the closest source of water and it distance from slums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterways = gpd.read_file(\"nairobi/osm_data/waterways.shp\")\n",
    "\n",
    "if gdf_slum.crs != waterways.crs:\n",
    "    waterways = waterways.to_crs(gdf_slum.crs)\n",
    "\n",
    "# Initialize lists to store results\n",
    "min_distances = []\n",
    "nearest_waterway_types = []\n",
    "\n",
    "# Iterate over each slum polygon\n",
    "for slum in gdf_slum['geometry']:\n",
    "    # Calculate distance to each waterway and store in a DataFrame\n",
    "    distances = waterways.distance(slum)\n",
    "    nearest_idx = distances.idxmin()  # Index of the nearest waterway\n",
    "    \n",
    "    # Get the minimum distance and type of the nearest waterway\n",
    "    min_distances.append(distances[nearest_idx])\n",
    "    nearest_waterway_types.append(waterways.loc[nearest_idx, 'type'])\n",
    "\n",
    "# Add results to the slums GeoDataFrame\n",
    "gdf_slum['min_distance_to_waterway'] = min_distances\n",
    "gdf_slum['nearest_waterway_type'] = nearest_waterway_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same logic for roads, railways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating NDVI using Sentinel 2 bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "# Define paths to the Red and NIR band files\n",
    "filepath = \"nairobi/sentinel-2/\"\n",
    "red_band_path = f\"{filepath}B04.jp2\"  # Band 4\n",
    "nir_band_path = f\"{filepath}B08.jp2\"  # Band 8\n",
    "\n",
    "\n",
    "# Open the Red and NIR bands as raster files\n",
    "with rasterio.open(red_band_path) as red_src:\n",
    "    red = red_src.read(1).astype('float32')  # Read the Red band and convert to float32\n",
    "    profile = red_src.profile  # Store metadata for writing the output NDVI\n",
    "\n",
    "with rasterio.open(nir_band_path) as nir_src:\n",
    "    nir = nir_src.read(1).astype('float32')  # Read the NIR band and convert to float32\n",
    "\n",
    "# Calculate NDVI using the formula\n",
    "ndvi = (nir - red) / (nir + red)\n",
    "\n",
    "# Handle NaN values (optional, e.g., if there are divide-by-zero results)\n",
    "ndvi = np.where(np.isfinite(ndvi), ndvi, 0)\n",
    "\n",
    "# Update profile to store NDVI as a single-band raster with float32 data type\n",
    "profile.update(dtype='float32', count=1, driver=\"GTiff\")\n",
    "\n",
    "# Save the NDVI to a new GeoTIFF file\n",
    "ndvi_output_path = \"nairobi/ndvi_output.tif\"\n",
    "with rasterio.open(ndvi_output_path, 'w', **profile) as dst:\n",
    "    dst.write(ndvi, 1)\n",
    "\n",
    "print(\"NDVI calculation complete. Output saved to\", ndvi_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating slope from elevation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "\n",
    "# keep the elevation map in local projection for nairobi it's 32737\n",
    "dem_path = \"nairobi/elevation.tif\"\n",
    "slope_path = \"nairobi/slope.tif\"\n",
    "ds = gdal.DemProcessing(dem_path, slope_path, processing=\"slope\")\n",
    "ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zonal Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating ndvi and slope gradient for each slum settlements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "ndvi_raster_path = \"nairobi/ndvi.tif\"\n",
    "\n",
    "# Load the slum polygons\n",
    "slums = gdf_slum\n",
    "\n",
    "print(slums.crs)\n",
    "# Define the green cover threshold for NDVI\n",
    "green_threshold = 0.3\n",
    "\n",
    "def percentage_green_cover(x):\n",
    "    # Remove NaN values from x\n",
    "    x = x[~np.isnan(x)]\n",
    "    # Calculate percentage of pixels above threshold\n",
    "    return (x > 0.3).sum() / len(x) * 100 if len(x) > 0 else 0\n",
    "\n",
    "\n",
    "# Calculate zonal statistics for NDVI within each polygon, using a categorical threshold to count green cover pixels\n",
    "stats = zonal_stats(\n",
    "    slums,\n",
    "    ndvi_raster_path,\n",
    "    stats=[\"mean\"],  # Calculate mean NDVI (optional, for reference)\n",
    "    add_stats={'green_cover': percentage_green_cover},\n",
    "    geojson_out=True\n",
    ")\n",
    "\n",
    "# Convert the zonal statistics output to a GeoDataFrame\n",
    "slums_with_ndvi = gpd.GeoDataFrame.from_features(stats)\n",
    "\n",
    "# Rename columns for clarity\n",
    "slums_with_ndvi = slums_with_ndvi.rename(columns={\"mean\": \"mean_ndvi\", \"green_cover\": \"percent_green_cover\"})\n",
    "\n",
    "# Convert green cover fraction to percentage\n",
    "slums_with_ndvi[\"percent_green_cover\"] = slums_with_ndvi[\"percent_green_cover\"] * 100\n",
    "\n",
    "# Display the result\n",
    "print(slums_with_ndvi[[\"mean_ndvi\", \"percent_green_cover\", \"geometry\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same logic for slope map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
