{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing spatial characteristics of informal settlement in Nairobi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sentinel 2 L1C\n",
    "2. OSM Boundaries\n",
    "3. Slum Boundaries\n",
    "4. Google Building Footprints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the slum boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_slum = gpd.read_file(\"nairobi/slum-boundaries/slums_Nrb.shp\")\n",
    "gdf_slum.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the CRS and attributes in the shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CRS of the slum settlement shapefile\", gdf_slum.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attributes\")\n",
    "print(gdf_slum.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Area of Slum Settlements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_slum[\"area_acre\"] = gdf_slum[\"geometry\"].area / 4047\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the biggest 10 landfill settlement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_slum = gdf_slum.loc[gdf_slum[\"area_acre\"].nlargest(10).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the OSM buildings layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_osm_buildings = gpd.read_file(\"nairobi/osm/buildings.shp\")\n",
    "gdf_osm_buildings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting buildings within slum settlements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure building shapefile has the same crs as slums\n",
    "gdf_osm_buildings_reproject = gdf_osm_buildings.to_crs(gdf_slum.crs)\n",
    "\n",
    "# apply spatial inner join with within query to get list of buildings within each slum\n",
    "# resuls in a dataframe with a new column to buildings called object id of slums\n",
    "buildings_within_slums = gpd.sjoin(gdf_osm_buildings_reproject, gdf_slum[[\"OBJECTID\", \"geometry\"]],\n",
    "                how=\"inner\",\n",
    "                predicate=\"within\",\n",
    "                lsuffix=\"left\",\n",
    "                rsuffix=\"right\")\n",
    "\n",
    "# Group by slum ID and count the buildings\n",
    "building_counts = buildings_within_slums.groupby(\"OBJECTID\").size().reset_index(name=\"building_count\")\n",
    "\n",
    "# finally merge building counts with slums \n",
    "slums_with_counts = gdf_slum.merge(building_counts, on=\"OBJECTID\", how=\"left\")\n",
    "\n",
    "# to calculate building density (number of buildings per acre)\n",
    "slums_with_counts[\"building_density\"] = slums_with_counts[\"building_count\"] / slums_with_counts[\"area_acre\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building footprint data can be too large to fit into memory, try loading data in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read buildings CSV in chunks and process each chunk\n",
    "def count_buildings_in_slums(slums, buildings_shp, crs, chunk_size=100000):\n",
    "    building_counts = gpd.GeoDataFrame()  # Initialize empty GeoDataFrame to store counts\n",
    "    \n",
    "    # Process buildings in chunks\n",
    "    for chunk in gpd.read_file(buildings_shp, chunksize=chunk_size):\n",
    "        # reproject the chunk to match slums crs\n",
    "        chunk = chunk.to_crs(crs)\n",
    "        \n",
    "        # Spatial join to find buildings within each slum\n",
    "        buildings_within_slums = gpd.sjoin(chunk, slums[[\"OBJECTID\", \"geometry\"]], how=\"inner\", predicate=\"within\")\n",
    "        \n",
    "        # Count buildings per slum in the current chunk\n",
    "        chunk_counts = buildings_within_slums.groupby(\"OBJECTID\").size().reset_index(name=\"building_count\")\n",
    "        \n",
    "        # Append to the main building counts DataFrame\n",
    "        building_counts = pd.concat([building_counts, chunk_counts], ignore_index=True)\n",
    "    \n",
    "    # Aggregate counts across chunks\n",
    "    building_counts = building_counts.groupby(\"OBJECTID\").sum().reset_index()\n",
    "    return building_counts\n",
    "\n",
    "# Calculate building counts and merge with slums GeoDataFrame\n",
    "buildings_shp = \"nairobi/osm/buildings.shp\"\n",
    "building_counts = count_buildings_in_slums(gdf_slum, buildings_shp)\n",
    "slums_with_counts = gdf_slum.merge(building_counts, on=\"OBJECTID\", how=\"left\")\n",
    "\n",
    "# Fill NaNs and set building count as integer\n",
    "slums_with_counts[\"building_count\"] = slums_with_counts[\"building_count\"].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the OSM points layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gdf = gpd.read_file(\"nairobi/osm/points.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding toilets within 500m of slum boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 500\n",
    "\n",
    "toilets = points_gdf[points_gdf[\"type\"]==\"toilets\"]\n",
    "\n",
    "# Create a 500m buffer around each slum polygon\n",
    "gdf_slum['geometry'] = gdf_slum['geometry'].buffer(buffer_size)\n",
    "\n",
    "# Perform spatial join to find toilets within each buffered slum polygon\n",
    "joined_gdf = gpd.sjoin(toilets, gdf_slum[[\"OBJECTID\", \"geometry\"]], how='inner', predicate='within')\n",
    "\n",
    "# Count the toilets for each slum polygon\n",
    "counts = joined_gdf.groupby('OBJECTID').size()\n",
    "\n",
    "# Add the counts back to the slum_gdf\n",
    "gdf_slum[f\"toilet_within_{buffer_size}m\"] = gdf_slum.OBJECTID.map(counts).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same logic for bus stops, hospitals and schools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the OSM line layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the closest source of water and it distance from slums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterways = gpd.read_file(\"nairobi/osm_data/waterways.shp\")\n",
    "\n",
    "if gdf_slum.crs != waterways.crs:\n",
    "    waterways = waterways.to_crs(gdf_slum.crs)\n",
    "\n",
    "# Initialize lists to store results\n",
    "min_distances = []\n",
    "nearest_waterway_types = []\n",
    "\n",
    "# Iterate over each slum polygon\n",
    "for slum in gdf_slum['geometry']:\n",
    "    # Calculate distance to each waterway and store in a DataFrame\n",
    "    distances = waterways.distance(slum)\n",
    "    nearest_idx = distances.idxmin()  # Index of the nearest waterway\n",
    "    \n",
    "    # Get the minimum distance and type of the nearest waterway\n",
    "    min_distances.append(distances[nearest_idx])\n",
    "    nearest_waterway_types.append(waterways.loc[nearest_idx, 'type'])\n",
    "\n",
    "# Add results to the slums GeoDataFrame\n",
    "gdf_slum['min_distance_to_waterway'] = min_distances\n",
    "gdf_slum['nearest_waterway_type'] = nearest_waterway_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same logic for roads, railways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating NDVI using Sentinel 2 bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "# Define paths to the Red and NIR band files\n",
    "filepath = \"nairobi/sentinel-2/\"\n",
    "red_band_path = f\"{filepath}B04.jp2\"  # Band 4\n",
    "nir_band_path = f\"{filepath}B08.jp2\"  # Band 8\n",
    "\n",
    "\n",
    "# Open the Red and NIR bands as raster files\n",
    "with rasterio.open(red_band_path) as red_src:\n",
    "    red = red_src.read(1).astype('float32')  # Read the Red band and convert to float32\n",
    "    profile = red_src.profile  # Store metadata for writing the output NDVI\n",
    "\n",
    "with rasterio.open(nir_band_path) as nir_src:\n",
    "    nir = nir_src.read(1).astype('float32')  # Read the NIR band and convert to float32\n",
    "\n",
    "# Calculate NDVI using the formula\n",
    "ndvi = (nir - red) / (nir + red)\n",
    "\n",
    "# Handle NaN values (optional, e.g., if there are divide-by-zero results)\n",
    "ndvi = np.where(np.isfinite(ndvi), ndvi, 0)\n",
    "\n",
    "# Update profile to store NDVI as a single-band raster with float32 data type\n",
    "profile.update(dtype='float32', count=1, driver=\"GTiff\")\n",
    "\n",
    "# Save the NDVI to a new GeoTIFF file\n",
    "ndvi_output_path = \"nairobi/ndvi_output.tif\"\n",
    "with rasterio.open(ndvi_output_path, 'w', **profile) as dst:\n",
    "    dst.write(ndvi, 1)\n",
    "\n",
    "print(\"NDVI calculation complete. Output saved to\", ndvi_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating slope from elevation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "\n",
    "# keep the elevation map in local projection for nairobi it's 32737\n",
    "dem_path = \"nairobi/elevation.tif\"\n",
    "slope_path = \"nairobi/slope.tif\"\n",
    "ds = gdal.DemProcessing(dem_path, slope_path, processing=\"slope\")\n",
    "ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zonal Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating ndvi and slope gradient for each slum settlements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "ndvi_raster_path = \"nairobi/ndvi.tif\"\n",
    "\n",
    "# Load the slum polygons\n",
    "slums = gdf_slum\n",
    "\n",
    "print(slums.crs)\n",
    "# Define the green cover threshold for NDVI\n",
    "green_threshold = 0.3\n",
    "\n",
    "def percentage_green_cover(x):\n",
    "    # Remove NaN values from x\n",
    "    x = x[~np.isnan(x)]\n",
    "    # Calculate percentage of pixels above threshold\n",
    "    return (x > 0.3).sum() / len(x) * 100 if len(x) > 0 else 0\n",
    "\n",
    "\n",
    "# Calculate zonal statistics for NDVI within each polygon, using a categorical threshold to count green cover pixels\n",
    "stats = zonal_stats(\n",
    "    slums,\n",
    "    ndvi_raster_path,\n",
    "    stats=[\"mean\"],  # Calculate mean NDVI (optional, for reference)\n",
    "    add_stats={'green_cover': percentage_green_cover},\n",
    "    geojson_out=True\n",
    ")\n",
    "\n",
    "# Convert the zonal statistics output to a GeoDataFrame\n",
    "slums_with_ndvi = gpd.GeoDataFrame.from_features(stats)\n",
    "\n",
    "# Rename columns for clarity\n",
    "slums_with_ndvi = slums_with_ndvi.rename(columns={\"mean\": \"mean_ndvi\", \"green_cover\": \"percent_green_cover\"})\n",
    "\n",
    "# Convert green cover fraction to percentage\n",
    "slums_with_ndvi[\"percent_green_cover\"] = slums_with_ndvi[\"percent_green_cover\"] * 100\n",
    "\n",
    "# Display the result\n",
    "print(slums_with_ndvi[[\"mean_ndvi\", \"percent_green_cover\", \"geometry\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same logic for slope map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texture Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "# List of filenames for each Sentinel-2 band\n",
    "filenames = {\n",
    "    \"B01\": \"T37MBU_20160802T075222_B01.jp2\",\n",
    "    \"B02\": \"T37MBU_20160802T075222_B02.jp2\",\n",
    "    \"B03\": \"T37MBU_20160802T075222_B03.jp2\",\n",
    "    \"B04\": \"T37MBU_20160802T075222_B04.jp2\",\n",
    "    \"B05\": \"T37MBU_20160802T075222_B05.jp2\",\n",
    "    \"B06\": \"T37MBU_20160802T075222_B06.jp2\",\n",
    "    \"B07\": \"T37MBU_20160802T075222_B07.jp2\",\n",
    "    \"B08\": \"T37MBU_20160802T075222_B08.jp2\",\n",
    "    \"B09\": \"T37MBU_20160802T075222_B09.jp2\",\n",
    "    \"B10\": \"T37MBU_20160802T075222_B10.jp2\",\n",
    "    \"B11\": \"T37MBU_20160802T075222_B11.jp2\",\n",
    "    \"B12\": \"T37MBU_20160802T075222_B12.jp2\",\n",
    "    \"B8A\": \"T37MBU_20160802T075222_B8A.jp2\"\n",
    "}\n",
    "\n",
    "# Load the slum boundaries shapefile\n",
    "slum_gdf = gpd.read_file('nairobi/informal-settlements.geojson')\n",
    "non_slum_gdf = gpd.read_file('nairobi/formal-settlements.geojson')\n",
    "\n",
    "slum_gdf = slum_gdf.to_crs(\"EPSG:32737\")\n",
    "non_slum_gdf = non_slum_gdf.to_crs(\"EPSG:32737\")\n",
    "\n",
    "# Initialize a dictionary to store mean values per band for slum and non-slum areas\n",
    "mean_values = {'Band': [], 'Slum_Area_Mean': [], 'Non_Slum_Area_Mean': []}\n",
    "\n",
    "# Loop through each file (band) in the filenames list\n",
    "file_prefix = \"/home/saheel/Documents/ITC/coursework/quartile-1/course-1/Data_CS_Informal Settlements/Nairobi data/Nrb_Sentinel_2A/S2A_MSIL1C_20160802T075222_N0204_R092_T37MBU_20160802T075556.SAFE/GRANULE/L1C_T37MBU_A005809_20160802T075556/IMG_DATA/\"\n",
    "for band_name, filename in filenames.items():\n",
    "    filename = file_prefix + filename\n",
    "    with rasterio.open(filename) as src:\n",
    "        # Read the data as a single band array\n",
    "        band = src.read(1)\n",
    "        \n",
    "        # Calculate mean pixel values within slum areas for this band\n",
    "        slum_stats = zonal_stats(slum_gdf, band, affine=src.transform, stats=\"mean\")\n",
    "        slum_mean = np.mean([stat['mean'] for stat in slum_stats if stat['mean'] is not None])\n",
    "        \n",
    "        # Calculate mean pixel values for the non-slum area (by excluding slum polygons)\n",
    "        non_slum_stats = zonal_stats(non_slum_gdf, band, affine=src.transform, stats=\"mean\")\n",
    "        non_slum_mean = np.mean([stat['mean'] for stat in non_slum_stats if stat['mean'] is not None])\n",
    "        \n",
    "        # Append results\n",
    "        mean_values['Band'].append(band_name)\n",
    "        mean_values['Slum_Area_Mean'].append(slum_mean)\n",
    "        mean_values['Non_Slum_Area_Mean'].append(non_slum_mean)\n",
    "\n",
    "# Convert to DataFrame for easy viewing and exporting\n",
    "mean_df = pd.DataFrame(mean_values)\n",
    "print(mean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(16,10))\n",
    "\n",
    "# Define the actual wavelengths for each band as x-ticks\n",
    "wavelengths = [0.443, 0.490, 0.560, 0.665, 0.705, 0.740, 0.783, 0.842, 0.865, 0.945, 1.375, 1.610, 2.190]\n",
    "band_labels = [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B10\", \"B11\", \"B12\"]\n",
    "\n",
    "sns.lineplot(data=mean_df, x=\"Wavelength\", y=\"Slum_Area_Mean\", ax=ax, color=\"blue\",marker=\"o\", label=\"Slum Area\")\n",
    "sns.lineplot(data=mean_df, x=\"Wavelength\", y=\"Non_Slum_Area_Mean\", ax=ax, color=\"red\",marker=\"o\", label=\"Non Slum Area\")\n",
    "\n",
    "# Set custom x-ticks\n",
    "ax.set_xticks(wavelengths)\n",
    "ax.set_xticklabels(band_labels, rotation=45, fontweight=\"bold\")\n",
    "# Adding labels and title\n",
    "ax.set_xlabel(\"Band Wavelength (µm)\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Mean Pixel Value\", fontweight=\"bold\")\n",
    "ax.set_title(\"Mean Pixel Values for Slum and Non-Slum Areas by Wavelength in Nairobi\", fontweight=\"bold\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading data from osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "\n",
    "# Define the bounding box (minx, miny, maxx, maxy) in latitude and longitude\n",
    "bbox = (-23.0329999, -43.7329996, -22.6320021, -43.078)\n",
    "\n",
    "# Download road network within the bounding box\n",
    "roads = ox.graph_from_bbox(bbox[2], bbox[0], bbox[3], bbox[1], network_type=\"all\")\n",
    "\n",
    "# Convert the road network to a GeoDataFrame\n",
    "edges = ox.graph_to_gdfs(roads, nodes=False, edges=True)\n",
    "\n",
    "# Filter the roads to keep only major roads if desired (optional)\n",
    "# edges = edges[edges[\"highway\"].isin([\"primary\", \"secondary\", \"tertiary\"])]\n",
    "\n",
    "# Save the roads data to a shapefile\n",
    "edges.to_file(\"osm_roads_within_bbox.shp\")\n",
    "\n",
    "# Print the result to confirm\n",
    "print(edges.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting data distribution using boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plot box plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Settlement Type', y='Building Density', hue=\"Region\", data=gdf_slum)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Building Density')\n",
    "plt.title('Informal and Formal Settlement Building Densities')\n",
    "plt.savefig(\"nairobi/visualisation/nairobi-rio_formal_informal_building_density.png\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
